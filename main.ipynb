{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "131423d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from document_ingestion import ingest_documents\n",
    "from audio_processing import transcribe_audio\n",
    "from video_processing import extract_frames\n",
    "from vector_search import search_text\n",
    "\n",
    "# 1️⃣ Ingest documents\n",
    "ingest_documents()\n",
    "\n",
    "# 2️⃣ Text query\n",
    "results = search_text(\"Explain self-attention in transformers\")\n",
    "print(results)\n",
    "\n",
    "# 3️⃣ Audio → Text → Search\n",
    "transcript = transcribe_audio(\"data/audio/sample.wav\")\n",
    "print(search_text(transcript))\n",
    "\n",
    "# 4️⃣ Video → Frames\n",
    "extract_frames(\"data/video/sample.mp4\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c85b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import uuid\n",
    "import re\n",
    "from enum import Enum\n",
    "from typing import List\n",
    "from neo4j import GraphDatabase\n",
    "\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain.schema import Document, BaseRetriever\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_huggingface import HuggingFaceEmbeddings  # ✅ Updated import\n",
    "\n",
    "from config import VECTOR_DB_PATH, EMBEDDING_MODEL, NEO4J_URI, NEO4J_USER, NEO4J_PASSWORD\n",
    "\n",
    "__all__ = [\"llm\", \"multimodal_search\", \"rerank_with_llm\", \"save_to_neo4j\", \"run_multimodal_qa\"]\n",
    "\n",
    "# Set API Keys\n",
    "if os.getenv(\"GROQ_API_KEY\"):\n",
    "    os.environ[\"GROQ_API_KEY\"] = os.getenv(\"GROQ_API_KEY\")\n",
    "if os.getenv(\"OPENAI_API_KEY\"):\n",
    "    os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# Embeddings\n",
    "from transformers.utils import logging\n",
    "logging.set_verbosity_error()  # hides transformers warnings\n",
    "\n",
    "embeddings = OpenAIEmbeddings(model=EMBEDDING_MODEL)\n",
    "image_embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/clip-ViT-B-32\")\n",
    "\n",
    "# Neo4j driver\n",
    "driver = GraphDatabase.driver(NEO4J_URI, auth=(NEO4J_USER, NEO4J_PASSWORD))\n",
    "\n",
    "# Optional verbose flag\n",
    "VERBOSE = False\n",
    "\n",
    "def load_chroma_if_exists(path, embedding_func):\n",
    "    \"\"\"Load Chroma DB only if it exists & has data.\"\"\"\n",
    "    if os.path.exists(path) and os.listdir(path):\n",
    "        return Chroma(persist_directory=path, embedding_function=embedding_func)\n",
    "    if VERBOSE:\n",
    "        print(f\"⚠️ Skipping vector store at {path} (not found or empty)\")\n",
    "    return None\n",
    "\n",
    "# Vector stores\n",
    "text_video_store = load_chroma_if_exists(VECTOR_DB_PATH, embeddings)\n",
    "audio_store = load_chroma_if_exists(os.path.join(VECTOR_DB_PATH, \"audio_db\"), embeddings)\n",
    "image_store = load_chroma_if_exists(os.path.join(VECTOR_DB_PATH, \"image_db\"), image_embeddings)\n",
    "\n",
    "def reciprocal_rank_fusion(tv_results, audio_results, image_results, k: int = 10) -> List[Document]:\n",
    "    combined = {}\n",
    "    for source_name, results in [\n",
    "        (\"Document\", tv_results),\n",
    "        (\"Audio\", audio_results),\n",
    "        (\"Image\", image_results)\n",
    "    ]:\n",
    "        for rank, (doc, score) in enumerate(results, start=1):\n",
    "            rr = 1 / (rank + 60)\n",
    "            key = doc.page_content\n",
    "            if key not in combined:\n",
    "                combined[key] = {\"doc\": doc, \"score\": 0}\n",
    "            combined[key][\"score\"] += rr\n",
    "            combined[key][\"doc\"].metadata[\"source_type\"] = source_name\n",
    "    sorted_docs = sorted(combined.values(), key=lambda x: x[\"score\"], reverse=True)\n",
    "    return [item[\"doc\"] for item in sorted_docs[:k]]\n",
    "\n",
    "def multimodal_search(query: str, k: int = 10, threshold: float = 0.3) -> List[Document]:\n",
    "    tv = [(d, s) for d, s in (text_video_store.similarity_search_with_score(query, k=k) if text_video_store else []) if s >= threshold]\n",
    "    aud = [(d, s) for d, s in (audio_store.similarity_search_with_score(query, k=k) if audio_store else []) if s >= threshold]\n",
    "    img = [(d, s) for d, s in (image_store.similarity_search_with_score(query, k=k) if image_store else []) if s >= threshold]\n",
    "    return reciprocal_rank_fusion(tv, aud, img, k)\n",
    "\n",
    "def rerank_with_llm(query: str, docs: List[Document], top_n: int = 3, llm=None) -> List[Document]:\n",
    "    if not docs or not llm:\n",
    "        return docs[:top_n]\n",
    "    prompt = f\"You are a reranker. Query: {query}\\nDocuments:\\n\"\n",
    "    for i, doc in enumerate(docs, start=1):\n",
    "        snippet = doc.page_content.replace(\"\\n\", \" \")[:400]\n",
    "        prompt += f\"{i}. {snippet}\\n\"\n",
    "    prompt += \"\\nReturn top 3 document numbers in order.\"\n",
    "    resp = llm.invoke(prompt).content\n",
    "    try:\n",
    "        indices = [int(x) for x in re.findall(r\"\\d+\", resp)]\n",
    "        return [docs[i-1] for i in indices if 1 <= i <= len(docs)]\n",
    "    except:\n",
    "        return docs[:top_n]\n",
    "\n",
    "class MultimodalRetriever(BaseRetriever):\n",
    "    def __init__(self, rerank_llm):\n",
    "        super().__init__()\n",
    "        self._rerank_llm = rerank_llm\n",
    "\n",
    "    def _get_relevant_documents(self, query: str) -> List[Document]:\n",
    "        docs = multimodal_search(query)\n",
    "        return rerank_with_llm(query, docs, llm=self._rerank_llm)\n",
    "\n",
    "def summarize_history(history_msgs, llm) -> str:\n",
    "    if not history_msgs:\n",
    "        return \"We haven’t had any conversation yet in this session.\"\n",
    "    history_text = \"\\n\".join(\n",
    "        (f\"User: {m.content}\" if getattr(m, 'type', '') == 'human' else f\"Bot: {m.content or m.page_content}\")\n",
    "        for m in history_msgs[-10:]\n",
    "    )\n",
    "    prompt = f\"Summarize the key topics from this conversation history:\\n{history_text}\\nProvide a concise summary.\"\n",
    "    return llm.invoke(prompt).content.strip()\n",
    "\n",
    "def create_chain(llm):\n",
    "    memory = ConversationBufferMemory(\n",
    "        memory_key=\"chat_history\",\n",
    "        input_key=\"question\",\n",
    "        output_key=\"answer\",\n",
    "        return_messages=True\n",
    "    )\n",
    "    chain = ConversationalRetrievalChain.from_llm(\n",
    "        llm=llm,\n",
    "        retriever=MultimodalRetriever(rerank_llm=llm),\n",
    "        memory=memory,\n",
    "        condense_question_llm=llm,\n",
    "        return_source_documents=False\n",
    "    )\n",
    "    return chain, memory\n",
    "\n",
    "def save_to_neo4j(user_id: str, session_id: str, query: str, answer: str, sources: list[str]):\n",
    "    with driver.session() as session:\n",
    "        session.run(\n",
    "            \"\"\"\n",
    "            MERGE (u:User {id: $user_id})\n",
    "            MERGE (s:Session {id: $session_id, date: date()})\n",
    "            MERGE (u)-[:HAS_SESSION]->(s)\n",
    "            MERGE (q:Query {text: $query, timestamp: datetime()})\n",
    "            MERGE (a:Answer {text: $answer})\n",
    "            MERGE (s)-[:ASKED]->(q)-[:ANSWERED_BY]->(a)\n",
    "            \"\"\", parameters={\"user_id\": user_id, \"session_id\": session_id, \"query\": query, \"answer\": answer}\n",
    "        )\n",
    "        for src in sources:\n",
    "            session.run(\n",
    "                \"\"\"\n",
    "                MERGE (src:Source {type: $type})\n",
    "                MERGE (a:Answer {text: $answer})\n",
    "                MERGE (a)-[:USED_SOURCE]->(src)\n",
    "                \"\"\", parameters={\"type\": src, \"answer\": answer}\n",
    "            )\n",
    "\n",
    "class QueryType(Enum):\n",
    "    HISTORY = \"history\"\n",
    "    FOLLOW_UP = \"follow_up\"\n",
    "    DOCUMENT = \"document\"\n",
    "\n",
    "def detect_query_type(query: str, history_msgs) -> QueryType:\n",
    "    text = query.lower().strip()\n",
    "    history_patterns = [\n",
    "        r\"what.*we.*\\b(discuss|talk|cover|speak|spoke)\\b\",\n",
    "        r\"on what topic.*we.*\\b(speak|spoke|discuss|covered)\\b\",\n",
    "        r\"what.*previous.*\\b(conversation|discussion)\\b\",\n",
    "        r\"tell me.*\\b(earlier|so far|before)\\b\",\n",
    "        r\"remind me\", r\"i forgot\",\n",
    "        r\"show me our \\b(chat|conversation|history)\\b\",\n",
    "        r\"what topics.*we.*(spoke|discussed)\"\n",
    "    ]\n",
    "    if any(re.search(p, text) for p in history_patterns):\n",
    "        return QueryType.HISTORY\n",
    "    follow_up_patterns = [r\"\\b(this|it|that|these|those)\\b\", r\"what about\", r\"what is the use of\", r\"how does it work\"]\n",
    "    if history_msgs and any(re.search(p, text) for p in follow_up_patterns):\n",
    "        return QueryType.FOLLOW_UP\n",
    "    return QueryType.DOCUMENT\n",
    "\n",
    "def rewrite_query_with_history(query: str, history_msgs, question_rewriter) -> str:\n",
    "    hist = []\n",
    "    for m in history_msgs[-6:]:\n",
    "        if getattr(m, 'type', '') == 'human':\n",
    "            hist.append(f\"User: {m.content}\")\n",
    "        else:\n",
    "            hist.append(f\"Bot: {m.content or m.page_content}\")\n",
    "    history_text = \"\\n\".join(hist)\n",
    "    prompt = f\"Conversation history:\\n{history_text}\\nRewrite the question '{query}' into a standalone question.\"\n",
    "    return question_rewriter.invoke(prompt).content.strip()\n",
    "\n",
    "def run_multimodal_qa(user_id: str, query: str, input_type: str, file_path: str = None, llm=None):\n",
    "    session_id = f\"{user_id}_{uuid.uuid4().hex[:6]}\"\n",
    "    qa_chain, memory = create_chain(llm)\n",
    "    history_msgs = memory.chat_memory.messages[-6:]\n",
    "\n",
    "    qtype = detect_query_type(query, history_msgs)\n",
    "\n",
    "    if qtype == QueryType.HISTORY:\n",
    "        summary = summarize_history(history_msgs, llm)\n",
    "        memory.chat_memory.add_user_message(query)\n",
    "        memory.chat_memory.add_ai_message(summary)\n",
    "        save_to_neo4j(user_id, session_id, query, summary, [\"ChatHistory\"])\n",
    "        return {\"answer\": summary, \"source\": \"summary\"}\n",
    "\n",
    "    final_query = rewrite_query_with_history(query, history_msgs, llm) if qtype == QueryType.FOLLOW_UP else query\n",
    "    docs = MultimodalRetriever(rerank_llm=llm)._get_relevant_documents(final_query)\n",
    "\n",
    "    if not docs:\n",
    "        answer = llm.invoke(final_query).content.strip()\n",
    "        memory.chat_memory.add_user_message(query)\n",
    "        memory.chat_memory.add_ai_message(answer)\n",
    "        save_to_neo4j(user_id, session_id, query, answer, [\"LLMOnly\"])\n",
    "        return {\"answer\": answer, \"source\": \"llm\"}\n",
    "\n",
    "    result = qa_chain({\"question\": final_query})\n",
    "    final_answer = result.get(\"answer\", \"\").strip()\n",
    "\n",
    "    if re.match(r\"(?i)i\\s+don'?t\\s+know\", final_answer):\n",
    "        final_answer = llm.invoke(final_query).content.strip()\n",
    "        memory.chat_memory.add_user_message(query)\n",
    "        memory.chat_memory.add_ai_message(final_answer)\n",
    "        save_to_neo4j(user_id, session_id, query, final_answer, [\"LLMOnly\"])\n",
    "        return {\"answer\": final_answer, \"source\": \"llm\"}\n",
    "\n",
    "    memory.chat_memory.add_user_message(query)\n",
    "    memory.chat_memory.add_ai_message(final_answer)\n",
    "    save_to_neo4j(user_id, session_id, query, final_answer, [])\n",
    "    return {\"answer\": final_answer, \"source\": \"retriever\"}\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Load an LLM for reranking\n",
    "    llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "    # Ask for user input\n",
    "    query = input(\"Enter your search query: \").strip()\n",
    "\n",
    "    # Run multimodal search\n",
    "    results = multimodal_search(query)\n",
    "\n",
    "    if not results:\n",
    "        print(\"❌ No results found in any modality.\")\n",
    "    else:\n",
    "        print(f\"\\n✅ Found {len(results)} results:\")\n",
    "        for i, doc in enumerate(results, start=1):\n",
    "            print(f\"\\n--- Result {i} ---\")\n",
    "            print(f\"Source Type: {doc.metadata.get('source_type', 'Unknown')}\")\n",
    "            print(f\"Content Preview: {doc.page_content[:200]}...\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bafc407d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "from PIL import Image\n",
    "import tempfile\n",
    "import os\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "from vector_search import (\n",
    "    create_chain,\n",
    "    save_to_neo4j,\n",
    "    detect_query_type,\n",
    "    summarize_history,\n",
    "    rewrite_query_with_history,\n",
    "    MultimodalRetriever,\n",
    "    run_multimodal_qa\n",
    ")\n",
    "from neo4j import GraphDatabase\n",
    "from config import NEO4J_URI, NEO4J_USER, NEO4J_PASSWORD\n",
    "\n",
    "# ------------------ Neo4j Load History ------------------\n",
    "def load_user_history_from_neo4j(user_id: str):\n",
    "    query = \"\"\"\n",
    "    MATCH (u:User {id: $user_id})-[:HAS_SESSION]->(:Session)-[:ASKED]->(q:Query)-[:ANSWERED_BY]->(a:Answer)\n",
    "    RETURN q.text AS question, a.text AS answer\n",
    "    ORDER BY q.timestamp\n",
    "    \"\"\"\n",
    "    driver = GraphDatabase.driver(NEO4J_URI, auth=(NEO4J_USER, NEO4J_PASSWORD))\n",
    "    with driver.session() as session:\n",
    "        results = session.run(query, parameters={\"user_id\": user_id})\n",
    "        return [(record[\"question\"], record[\"answer\"]) for record in results]\n",
    "\n",
    "# ------------------ Session State Setup ------------------\n",
    "if \"user_id\" not in st.session_state:\n",
    "    st.session_state.user_id = None\n",
    "if \"page\" not in st.session_state:\n",
    "    st.session_state.page = \"start\"\n",
    "if \"chat_history\" not in st.session_state:\n",
    "    st.session_state.chat_history = []\n",
    "\n",
    "# ------------------ Page 1: Landing ------------------\n",
    "if st.session_state.page == \"start\":\n",
    "    st.title(\"Multimodal Retrieval-Augmented Generation (RAG) Assistant\")\n",
    "    st.markdown(\"\"\"\n",
    "    This system integrates document, audio, image, and video data into a unified RAG framework for enhanced contextual understanding.\n",
    "    It supports LLM-based reasoning with configurable settings and personalized chat history via Neo4j.\n",
    "    \"\"\")\n",
    "\n",
    "    # Project Info Button\n",
    "    with st.expander(\"Project Info\", expanded=False):\n",
    "        st.markdown(\"\"\"\n",
    "        ### Multimodal RAG Assistant – Project Description\n",
    "\n",
    "        This assistant integrates **text, audio, image, and video data** into a unified **Retrieval-Augmented Generation (RAG)** system. It enhances user queries using document search, LLM reasoning, and multimodal embeddings to provide accurate and context-aware answers.\n",
    "\n",
    "        #### Concepts and Tools Used:\n",
    "        - **LangChain**: For chaining LLMs and retrievers in a modular pipeline.\n",
    "        - **ChromaDB**: Vector database to store text, audio, image, and video embeddings.\n",
    "        - **OpenAI Whisper**: For transcribing audio/video files.\n",
    "        - **CLIP**: To generate image embeddings.\n",
    "        - **OpenAI / Groq LLMs**: For answering queries and reranking results.\n",
    "        - **Neo4j**: To store and retrieve personalized chat history.\n",
    "        - **RRF (Reciprocal Rank Fusion)**: Combines results from different modalities (text, audio, image, video).\n",
    "        - **LLM-based Reranking**: Ensures only high-quality relevant results are shown.\n",
    "\n",
    "        #### Workflow Summary:\n",
    "        1. **Input**: User provides a query (text) or uploads an image/audio/video file.\n",
    "        2. **Classification**: Determines whether the query is new, a follow-up, or requires summarization.\n",
    "        3. **Multimodal Retrieval**: Fetches relevant documents from ChromaDB using embeddings.\n",
    "        4. **Reranking**: Uses LLMs to rerank retrieved results based on relevance.\n",
    "        5. **Answer Generation**: Uses QA chains or fallback LLM generation.\n",
    "        6. **History Storage**: Neo4j logs user-specific query-answer pairs.\n",
    "        \"\"\", unsafe_allow_html=True)\n",
    "\n",
    "    user_id = st.text_input(\"Enter your User ID\", key=\"user_id_input\")\n",
    "    st.markdown(\"---\")\n",
    "    st.markdown(\"### Model Settings\")\n",
    "\n",
    "    model_choice = st.selectbox(\"Choose LLM Model\", [\"OpenAI (gpt-4o-mini)\", \"Groq (llama3-70b-8192)\"])\n",
    "    temperature = st.slider(\"Set Temperature\", 0.0, 1.0, 0.2, 0.05)\n",
    "    max_tokens = st.slider(\"Max Tokens\", 128, 4096, 1024, 64)\n",
    "\n",
    "    if \"OpenAI\" in model_choice:\n",
    "        st.markdown(\"*`GPT-4o-mini` supports ~128k tokens context, recommended max generation: ~2048–4096 tokens.*\")\n",
    "    else:\n",
    "        st.markdown(\"*`LLaMA3-70B` supports ~8k tokens context, recommended max generation: ~2048–4096 tokens.*\")\n",
    "\n",
    "    if st.button(\"Continue\"):\n",
    "        if user_id.strip():\n",
    "            st.session_state.user_id = user_id.strip()\n",
    "            st.session_state.page = \"chat\"\n",
    "\n",
    "            if \"OpenAI\" in model_choice:\n",
    "                llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=temperature, max_tokens=max_tokens)\n",
    "            else:\n",
    "                llm = ChatGroq(model=\"llama3-70b-8192\", temperature=temperature, max_tokens=max_tokens)\n",
    "\n",
    "            st.session_state.llm = llm\n",
    "            st.session_state.qa_chain, st.session_state.memory = create_chain(llm)\n",
    "\n",
    "            past_chats = load_user_history_from_neo4j(user_id.strip())\n",
    "            for q, a in past_chats:\n",
    "                st.session_state.memory.chat_memory.add_user_message(q)\n",
    "                st.session_state.memory.chat_memory.add_ai_message(a)\n",
    "\n",
    "            st.rerun()\n",
    "        else:\n",
    "            st.warning(\"User ID cannot be empty.\")\n",
    "\n",
    "# ------------------ Page 2: Chat UI ------------------\n",
    "elif st.session_state.page == \"chat\":\n",
    "    st.title(\"Ask Your Question\")\n",
    "    st.markdown(f\"**User ID:** `{st.session_state.user_id}`\")\n",
    "\n",
    "    if st.button(\"Sign Out\"):\n",
    "        st.session_state.page = \"start\"\n",
    "        st.session_state.user_id = None\n",
    "        st.session_state.chat_history = []\n",
    "        del st.session_state.qa_chain\n",
    "        del st.session_state.memory\n",
    "        del st.session_state.llm\n",
    "        st.rerun()\n",
    "\n",
    "    col1, col2 = st.columns(2)\n",
    "    with col1:\n",
    "        input_type = st.radio(\"Choose input type\", [\"Text\", \"Image\", \"Audio\", \"Video\"], key=\"input_type_radio\")\n",
    "    with col2:\n",
    "        if st.button(\"Show History\"):\n",
    "            with st.expander(\"Conversation History\", expanded=True):\n",
    "                for item in st.session_state.chat_history:\n",
    "                    st.markdown(f\"**You:** {item['user']}\")\n",
    "                    label = item['source']\n",
    "                    label_text = (\n",
    "                        \"**Bot (Document):** \" if label == \"retriever\" else\n",
    "                        \"**LLM-only:** \" if label == \"llm\" else\n",
    "                        \"**Bot (summary):** \" if label == \"summary\" else\n",
    "                        \"**Bot:** \"\n",
    "                    )\n",
    "                    st.markdown(f\"{label_text} {item['bot']}\")\n",
    "            st.stop()\n",
    "\n",
    "    query = None\n",
    "    file_path = None\n",
    "\n",
    "    if input_type == \"Text\":\n",
    "        query = st.text_area(\"Type your question here\", height=100)\n",
    "    elif input_type == \"Image\":\n",
    "        uploaded_file = st.file_uploader(\"Upload an image\", type=[\"png\", \"jpg\", \"jpeg\"])\n",
    "        if uploaded_file:\n",
    "            image = Image.open(uploaded_file)\n",
    "            st.image(image, caption=\"Uploaded Image\", use_column_width=True)\n",
    "            query = st.text_area(\"Ask a question about this image\", height=100)\n",
    "            with tempfile.NamedTemporaryFile(delete=False, suffix=\".png\") as tmp:\n",
    "                image.save(tmp.name)\n",
    "                file_path = tmp.name\n",
    "    elif input_type == \"Audio\":\n",
    "        uploaded_file = st.file_uploader(\"Upload an audio file\", type=[\"mp3\", \"wav\"])\n",
    "        if uploaded_file:\n",
    "            st.audio(uploaded_file)\n",
    "            # No text box for audio; user only uploads audio\n",
    "            with tempfile.NamedTemporaryFile(delete=False, suffix=os.path.splitext(uploaded_file.name)[1]) as tmp:\n",
    "                tmp.write(uploaded_file.getbuffer())\n",
    "                file_path = tmp.name\n",
    "    elif input_type == \"Video\":\n",
    "        uploaded_file = st.file_uploader(\"Upload a video file\", type=[\"mp4\", \"mov\", \"avi\"])\n",
    "        if uploaded_file:\n",
    "            st.video(uploaded_file)\n",
    "            # No text box for video; user only uploads video\n",
    "            with tempfile.NamedTemporaryFile(delete=False, suffix=os.path.splitext(uploaded_file.name)[1]) as tmp:\n",
    "                tmp.write(uploaded_file.getbuffer())\n",
    "                file_path = tmp.name\n",
    "\n",
    "    if st.button(\"Submit\"):\n",
    "        if input_type == \"Text\":\n",
    "            if not query or not query.strip():\n",
    "                st.warning(\"Please enter a question.\")\n",
    "                st.stop()\n",
    "        else:\n",
    "            # For media inputs, text query is not mandatory\n",
    "            if not file_path:\n",
    "                st.warning(f\"Please upload a {input_type.lower()} file.\")\n",
    "                st.stop()\n",
    "\n",
    "        if input_type == \"Text\" and query.strip().lower() == \"exit\":\n",
    "            st.session_state.page = \"start\"\n",
    "            st.session_state.user_id = None\n",
    "            st.session_state.chat_history = []\n",
    "            del st.session_state.qa_chain\n",
    "            del st.session_state.memory\n",
    "            del st.session_state.llm\n",
    "            st.success(\"You have been logged out.\")\n",
    "            st.rerun()\n",
    "\n",
    "        with st.spinner(\"Processing...\"):\n",
    "            memory = st.session_state.memory\n",
    "            llm = st.session_state.llm\n",
    "            history_msgs = memory.chat_memory.messages[-6:]\n",
    "            # For media input, we transcribe file to text query\n",
    "            if input_type in [\"Audio\", \"Video\"]:\n",
    "                from openai import OpenAI\n",
    "                client = OpenAI()\n",
    "                with open(file_path, \"rb\") as f:\n",
    "                    transcript = client.audio.transcriptions.create(model=\"whisper-1\", file=f)\n",
    "                query = transcript.text.strip()\n",
    "                if not query:\n",
    "                    st.warning(f\"Could not transcribe the {input_type.lower()}. Please try again.\")\n",
    "                    st.stop()\n",
    "\n",
    "            qtype = detect_query_type(query if input_type == \"Text\" else query, history_msgs)\n",
    "\n",
    "            if qtype.name == \"HISTORY\":\n",
    "                summary = summarize_history(history_msgs, llm)\n",
    "                memory.chat_memory.add_user_message(query)\n",
    "                memory.chat_memory.add_ai_message(summary)\n",
    "                source = \"summary\"\n",
    "                answer = summary\n",
    "            else:\n",
    "                final_query = rewrite_query_with_history(query, history_msgs, llm) if qtype.name == \"FOLLOW_UP\" else query\n",
    "                retriever = MultimodalRetriever(rerank_llm=llm)\n",
    "                docs = retriever._get_relevant_documents(final_query)\n",
    "\n",
    "                if not docs:\n",
    "                    answer = llm.invoke(final_query).content.strip()\n",
    "                    memory.chat_memory.add_user_message(query)\n",
    "                    memory.chat_memory.add_ai_message(answer)\n",
    "                    save_to_neo4j(st.session_state.user_id, f\"{st.session_state.user_id}_streamlit\", query, answer, [\"LLMOnly\"])\n",
    "                    source = \"llm\"\n",
    "                else:\n",
    "                    result = st.session_state.qa_chain({\"question\": final_query})\n",
    "                    answer = result.get(\"answer\", \"\").strip()\n",
    "\n",
    "                    if \"i don't know\" in answer.lower():\n",
    "                        answer = llm.invoke(final_query).content.strip()\n",
    "                        memory.chat_memory.add_user_message(query)\n",
    "                        memory.chat_memory.add_ai_message(answer)\n",
    "                        save_to_neo4j(st.session_state.user_id, f\"{st.session_state.user_id}_streamlit\", query, answer, [\"LLMOnly\"])\n",
    "                        source = \"llm\"\n",
    "                    else:\n",
    "                        memory.chat_memory.add_user_message(query)\n",
    "                        memory.chat_memory.add_ai_message(answer)\n",
    "                        save_to_neo4j(st.session_state.user_id, f\"{st.session_state.user_id}_streamlit\", query, answer, [])\n",
    "                        source = \"retriever\"\n",
    "\n",
    "            st.markdown(f\"**You:** {query if input_type == 'Text' else f'[{input_type} file uploaded]'}\")\n",
    "            label_text = (\n",
    "                \"**Bot (Document):** \" if source == \"retriever\" else\n",
    "                \"**LLM-only:** \" if source == \"llm\" else\n",
    "                \"**Bot (summary):** \" if source == \"summary\" else\n",
    "                \"**Bot:** \"\n",
    "            )\n",
    "            st.markdown(f\"{label_text} {answer}\")\n",
    "\n",
    "            st.session_state.chat_history.append({\n",
    "                \"user\": query if input_type == \"Text\" else f\"[{input_type} file uploaded]\",\n",
    "                \"bot\": answer,\n",
    "                \"source\": source\n",
    "            })\n",
    "\n",
    "        # Clean up temp file if used\n",
    "        if file_path and os.path.exists(file_path):\n",
    "            os.remove(file_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

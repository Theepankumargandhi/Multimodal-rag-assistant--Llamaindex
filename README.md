# ðŸ¦™ Multimodal Retrieval-Augmented Generation (RAG) Assistant

A **Streamlit-based AI assistant** that integrates **text, audio, image, and video data** into a unified **Retrieval-Augmented Generation (RAG)** framework for **context-aware answers**.  
The system combines **LlamaIndex**, **LangChain**, and **Neo4j Graph-RAG** to deliver precise, explainable, and multimodal insights with persistent conversation history.

---

## ðŸš€ Features

- **Multimodal Input Support** â€“ Query using text, upload audio/video (transcribed with Whisper), or images (embedded with CLIP).
- **Hybrid RAG Engine** â€“ Combines LlamaIndex (retrieval) and LangChain (orchestration, memory).
- **Graph-RAG Enrichment** â€“ Enhances answers using relationship facts stored in **Neo4j** (e.g., *â€œHow is OpenAI connected to Microsoft?â€*).
- **LLM-Based Reranking** â€“ Uses Reciprocal Rank Fusion (RRF) and reranking for better context relevance.
- **Persistent Chat History** â€“ All sessions are saved to Neo4j for context-aware follow-ups.
- **Dynamic Model Selection**
  - `GPT-4o-mini` (OpenAI) â€“ ~128k context
  - `LLaMA-3.3-70B-Versatile` (Groq) â€“ ~8k context
- **Configurable UI Controls** â€“ Toggle Graph-RAG, choose model, adjust temperature and max tokens.
- **Streamlit Frontend** â€“ Clean, modern interface with media upload, chat history viewer, and user session tracking.

---
## ðŸ§  Tech Stack

| Layer | Technology |
|-------|-------------|
| **Frontend** | Streamlit |
| **Backend** | Python, LangChain, LlamaIndex |
| **Database** | ChromaDB (vector store) + Neo4j (graph & session history) |
| **Embeddings** | OpenAI (text/audio) + CLIP (images) |
| **LLMs** | OpenAI GPT-4o-mini, Groq LLaMA-3.3-70B |
| **Audio/Video** | Whisper (OpenAI) transcription |

---

## ðŸ—‚ Project Structure
```
multimodal-rag-assistant/
â”œâ”€â”€ app.py # Streamlit frontend (main UI)
â”œâ”€â”€ vector_search.py # Core hybrid RAG & Graph-RAG logic
â”œâ”€â”€ llama_index_setup.py # LlamaIndex configuration & retriever setup
â”œâ”€â”€ config.py # Central configuration & environment settings
â”œâ”€â”€ graph_enrichment.py # Triple extraction and Neo4j enrichment
â”œâ”€â”€ graph_queries.py # Querying relationships from Neo4j
â”œâ”€â”€ document_ingestion.py # Document ingestion (text embedding)
â”œâ”€â”€ image_ingestion.py # Image embedding via CLIP
â”œâ”€â”€ audio_ingestion.py # Audio transcription & embedding
â”œâ”€â”€ video_ingestion.py # Video â†’ audio â†’ text ingestion
â”œâ”€â”€ requirements.txt # Python dependencies
â”œâ”€â”€ .env.example # Environment variable template
â”œâ”€â”€ Screenshots/ # UI screenshots
â”‚ â”œâ”€â”€ landing_page.png
â”‚ â”œâ”€â”€ selection.png
â”‚ â”œâ”€â”€ audio_output.png
â”‚ â”œâ”€â”€ image_output.png
â”‚ â”œâ”€â”€ text_output.png
â”‚ â””â”€â”€ summary.png
â””â”€â”€ README.md
```

---

## ðŸ§© Workflow Overview

1. **User Input**  
   Text, image, audio, or video input provided via Streamlit UI.  

2. **Query Classification**  
   System detects if itâ€™s a history query, follow-up, or new query.  

3. **Ingestion & Indexing**  
   - Text â†’ OpenAI embeddings  
   - Image â†’ CLIP embeddings  
   - Audio/Video â†’ Whisper â†’ text â†’ embeddings  

4. **Multimodal Retrieval (LlamaIndex)**  
   Retrieves top-k results from Chroma vector store across all modalities.

5. **RRF + LLM Reranking (LangChain)**  
   Combines and reranks results for best relevance.

6. **Graph-RAG Enrichment (Neo4j)**  
   Extracts entity-relation triples (E1-REL-E2) and adds relational reasoning.  

7. **Answer Generation**  
   Synthesizes the final grounded answer via LLM.

8. **Session Persistence**  
   Logs Q&A and sources into Neo4j (for contextual follow-ups).

---

## ðŸ–¼ Screenshots

**Landing Page**  
![Landing Page](Screenshots/landing_page.png)

**Input Selection**  
![Selection Page](Screenshots/selection.png)

**Audio Query Output**  
![Audio Output](Screenshots/audio_output.png)

**Image Query Output**  
![Image Output](Screenshots/image_output.png)

**Text Query Output**  
![Text Output](Screenshots/text_output.png)

**Conversation Summary**  
![Chat Summary](Screenshots/summary.png)

---
## âš™ï¸ Installation Guide

### 1ï¸âƒ£ Clone the Repository

```
git clone https://github.com/Theepankumargandhi/multimodal-rag-assistant--Llamaindex.git
```
```
cd multimodal-rag-assistant--Llamaindex
```
---
## Install Dependencies
```
pip install -r requirements.txt
```
## Configure Environment Variables
```
OPENAI_API_KEY=your-openai-key
GROQ_API_KEY=your-groq-key
NEO4J_URI=bolt://localhost:7687
NEO4J_USER=neo4j
NEO4J_PASSWORD=your-password
```
---
## Prepare Data
data/docs/        â†’ PDF/TXT documents
data/images/      â†’ JPG/PNG images
data/audio/       â†’ MP3/WAV audio
data/video/       â†’ MP4/MOV videos

## Run the ingestion scripts
```
python document_ingestion.py
python image_ingestion.py
python audio_ingestion.py
python video_ingestion.py
```
## Launch the App
```
streamlit run app.py
```
---
## Graph-RAG Mode

When Graph-RAG is enabled (toggle in sidebar):

Extracts and stores knowledge triples (Entity1 -[REL]-> Entity2) in Neo4j.
Answers are enriched with supporting graph facts for relational queries.

Example:
Q: â€œHow is OpenAI connected to Microsoft?â€
A: â€œOpenAI partnered with Microsoft to deploy models on Azure. (Graph Context: PARTNERS_WITH)â€

To inspect your graph manually in Neo4j:
```
MATCH (a:Entity)-[r:REL]->(b:Entity)
RETURN a.name, r.label, r.source_doc, r.chunk_id, b.name
LIMIT 25;
```